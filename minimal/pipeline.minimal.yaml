
---
# directories
pipeline_dir: '/beagle3/haky/users/temi/projects/TFPred-snakemake' # not necessary anymore
dataset: 'ENPACT_MINIMAL_FAST' # give this dataset or run a name e.g MY PREDICTIONS
date: "2024-09-12" # this date will be used to create the directory structure

scratch_dir: "/scratch/midway3/temi" # large files will be saved here; very important if you will be generating large files with Enformer
train_by_chromosome: # by default, we train and test on different chromosomes
  condition: True # add this but will be removed
  test: ['chr9', 'chr22'] # logically, training will the done on the other chromosomes

# data information
models_config: "minimal/models.data.yaml" ## these are the peaks and motifs to be used in training the models
models_metadata: "minimal/models.run.tsv" ## these are the models to be trained

# genome fasta file
genome:
  fasta: "/project/haky/data/hg_sequences/hg38/Homo_sapiens_assembly38.fasta"

# location of the peaks i.e. the bed files
peaks:
  directory: minimal/data/chippeaks # where are the bed files

# homer is used to find the motif instances
homer:
  dir: '/beagle3/haky/users/temi/software/homer' # directory to the homer software you, hopefully, have installed
  motifs_database: minimal/data/motifs # where are the .motif files
  scanMotifsGenome: '/beagle3/haky/users/temi/software/homer/bin/scanMotifGenomeWide.pl' # will be removed 
  genome: '/beagle3/haky/users/temi/software/homer/data/genomes/hg38' # should ideally be the same fasta file used in training (should be removed)

rscript: "/beagle3/haky/users/shared_software/TFXcan-pipeline-tools/bin/Rscript" # the Rscript

# directives to use fast or exhaustive pipeline
run_enformer: False # set to true if you want enformer to be run
delete_enformer_outputs: True # should you delete enformer predictions after aggregating them? Saves disk space; this is only used if run_enformer is True
enformer_epigenome_directory: '/beagle3/haky/data/enformer-reference-epigenome' # In case you have access to a directory with a reference epigenome data, you can use it here

# this won't be used if run_enformer is false since there is not need to run GPUs
enformer: 
  base_directives: "config/enformer_base.yaml"
  model: "/project/haky/data/enformer/raw"
  predict: "/beagle3/haky/users/shared_pipelines/enformer_pipeline_aggregate/scripts/enformer_predict.py"
  aggregate: "/beagle3/haky/users/shared_pipelines/enformer_pipeline_aggregate/scripts/aggregate/aggregate.py"
  aggtype: "aggByCollect"
  ngpus: 4 # shouldn't request more than 4; 2 should start to run quite quickly

# personalized:
#   directives: False
...
