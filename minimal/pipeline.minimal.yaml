
---
usage:
  offline: True # i.e you have donwloa
  software: "singularity" # or conda
  location: "/beagle3/haky/users/temi/projects/build_singularity/sif_files/tfxcan-v1.0.sif" # or the conda environment.yaml file

dataset: 'ENPACT_MINIMAL_FAST' # give this dataset or run a name e.g MY PREDICTIONS
date: "2024-09-18" # this date will be used to create the directory structure

scratch_dir: "/scratch/midway3/temi" # large files will be saved here; very important if you will be generating large files with Enformer
train_by_chromosome: # by default, we train and test on different chromosomes
  condition: True # add this but will be removed
  test: ['chr9', 'chr22'] # logically, training will the done on the other chromosomes

# data information
models_config: "minimal/models.data.yaml" ## these are the peaks and motifs to be used in training the models
models_metadata: "minimal/models.run.tsv" ## these are the models to be trained

# genome fasta file
genome:
  fasta: "files/Homo_sapiens_assembly38.fasta"
  chrom_sizes: "info/hg38.chrom.sizes.sorted"

# location of the peaks i.e. the bed files
peaks:
  directory: minimal/data/chippeaks # where are the bed files

# homer is used to find the motif instances; not compulsory to provide these if the singularity container is being used
# homer:
#   dir: '/beagle3/haky/users/temi/software/homer' # directory to the homer software you, hopefully, have installed
#   motifs_database: minimal/data/motifs # where are the .motif files
#   #scanMotifsGenome: 'bin/scanMotifGenomeWide.pl' # will be removed 
#   genome: 'data/genomes/hg38' # should ideally be the same fasta file used in training (should be removed)

rscript: Rscript # the  "/software/conda_envs/TFXcan-snakemake/bin/Rscript"

# directives to use fast or exhaustive pipeline
run_enformer: False # set to true if you want enformer to be run
delete_enformer_outputs: True # should you delete enformer predictions after aggregating them? Saves disk space; this is only used if run_enformer is True
enformer_epigenome_directory: '/beagle3/haky/data/enformer-reference-epigenome' # In case you have access to a directory with a reference epigenome data, you can use it here

# this won't be used if run_enformer is false since there is not need to run GPUs
enformer: 
  base_directives: "config/enformer_base.yaml"
  model: "/project/haky/data/enformer/raw"
  predict: "/beagle3/haky/users/shared_pipelines/enformer_pipeline_aggregate/scripts/enformer_predict.py"
  aggregate: "/beagle3/haky/users/shared_pipelines/enformer_pipeline_aggregate/scripts/aggregate/aggregate.py"
  aggtype: "aggByCollect"
  ngpus: 4 # shouldn't request more than 4; 2 should start to run quite quickly

# personalized:
#   directives: False
...
