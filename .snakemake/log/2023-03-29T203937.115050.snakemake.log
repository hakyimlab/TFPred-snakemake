Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job                 count    min threads    max threads
----------------  -------  -------------  -------------
all                     1              1              1
gather_bed_files        3              1              1
total                   4              1              1

Select jobs to execute...

[Wed Mar 29 20:39:37 2023]
Job 3: working on AR and Breast data/bed_files/AR_Breast
Reason: Missing output files: data/bed_files/AR_Breast

[Wed Mar 29 20:39:39 2023]
Finished job 3.
1 of 4 steps (25%) done
Select jobs to execute...

[Wed Mar 29 20:39:39 2023]
Job 1: working on FOXA1 and Breast data/bed_files/FOXA1_Breast
Reason: Missing output files: data/bed_files/FOXA1_Breast

[Wed Mar 29 20:39:40 2023]
Finished job 1.
2 of 4 steps (50%) done
Select jobs to execute...

[Wed Mar 29 20:39:40 2023]
Job 2: working on FOXA1 and Prostate data/bed_files/FOXA1_Prostate
Reason: Missing output files: data/bed_files/FOXA1_Prostate

[Wed Mar 29 20:39:42 2023]
Finished job 2.
3 of 4 steps (75%) done
Select jobs to execute...

[Wed Mar 29 20:39:42 2023]
localrule all:
    input: data/bed_files/FOXA1_Breast, data/bed_files/FOXA1_Prostate, data/bed_files/AR_Breast
    jobid: 0
    reason: Input files updated by another job: data/bed_files/FOXA1_Breast, data/bed_files/AR_Breast, data/bed_files/FOXA1_Prostate
    resources: tmpdir=/tmp

[Wed Mar 29 20:39:42 2023]
Finished job 0.
4 of 4 steps (100%) done
Complete log: .snakemake/log/2023-03-29T203937.115050.snakemake.log
