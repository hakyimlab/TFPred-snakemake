---
title: "Prepare Enpact models to train"
author: "Temi"
date: "2024-06-26"
---


```{r}
library(data.table)
library(magrittr)
library(yaml)
library(glue)
library(dplyr)
library(fpeek)
```

```{r}
# source('/beagle3/haky/users/temi/projects/TFPred-snakemake/misc/wc.R')
```

```{r}
# Homer gives you motifs to symbol
# homerdb <- data.table::fread('/project2/haky/temi/projects/TFPred-snakemake/metadata/motifs2symbol.txt', header = T)
# homerdb
```

```{r}
homerdb <- data.table::fread('/project2/haky/temi/projects/TFPred-snakemake/metadata/motifTable.txt', header = T) %>%
    dplyr::filter(!grepl(',', `Gene Symbol`), !`Gene Symbol` %in% c( "-", "?")) %>%
    dplyr::select(Filename, Consensus, symbol=`Gene Symbol`)
homerdb
```

```{r}
peaks_dir <- '/project/haky/data/TFXcan/cistrome/raw/human_factor'
```

```{r}
mt <- data.table::fread('/project2/haky/Data/TFXcan/cistrome/raw/human_factor_full_QC.txt') %>%
    dplyr::filter(Tissue_type != 'None', !is.na(PeaksUnionDHSRatio)) %>%
    dplyr::filter(!grepl('-', Factor, fixed = T)) %>%
    dplyr::filter(Factor %in% homerdb$symbol)

idt <- mt %>%
    dplyr::group_by(Factor, Tissue_type) %>%
    dplyr::group_split() 
```

```{r}

ldt <- list()

for(i in seq_along(idt)){
    fdt <- idt[[i]]
    tissuename <- unique(fdt$Tissue_type)
    tissuename <- gsub(' ', '', tissuename)
    tfname <- unique(fdt$Factor)
    dcids <- fdt$DCid

    if(!tfname %in% names(ldt)){
        ldt[[as.name(tfname)]] <- list()
        ldt[[as.name(tfname)]][['peakFiles']] <- list()
        ldt[[as.name(tfname)]][['peakFiles']][[as.name(tissuename)]] <-  paste0(dcids, '_sort_peaks.narrowPeak.bed')
        
    } else {
        ldt[[as.name(tfname)]][['peakFiles']][[as.name(tissuename)]] <-  paste0(dcids, '_sort_peaks.narrowPeak.bed')
    }
}

# ldt <- lapply(idt, function(eachdt){
#     eachdt <- eachdt %>% dplyr::arrange(desc(FRiP))
#     tissuename <- unique(eachdt$Tissue_type)
#     tissuename <- gsub(' ', '', tissuename)
#     tfname <- unique(eachdt$Factor)
#     dcids <- eachdt$DCid
#     et <- list()
#     et[[as.name(tfname)]] <- list()
#     et[[as.name(tfname)]][[as.name(tissuename)]] <- paste0(dcids, '_sort_peaks.narrowPeak.bed')
#     return(et)
# })



# for(i in seq_along(idt)){
#     fdt <- idt[[i]]
#     tissuename <- unique(fdt$Tissue_type)
#     tissuename <- gsub(' ', '', tissuename)
#     tfname <- unique(fdt$Factor)
#     dcids <- fdt$DCid

#     if(!tfname %in% names(ldt)){
#         ff <- paste0(dcids, '_sort_peaks.narrowPeak.bed')
#         pp <- file.path(peaks_dir, ff)
#         wcl <- sapply(pp, fpeek::peek_count_lines) # count how many peaks are present in this file
#         if(length(wcl) == 1){
#             if(wcl[1] < 200){
#                 next
#             } else if(wcl[1] >= 200) {
#                 ldt[[as.name(tfname)]] <- list()
#                 ldt[[as.name(tfname)]][['peakFiles']] <- list()
#                 ldt[[as.name(tfname)]][['peakFiles']][[as.name(tissuename)]] <-  paste0(dcids, '_sort_peaks.narrowPeak.bed')
#             }
#         else {
#             ldt[[as.name(tfname)]] <- list()
#             ldt[[as.name(tfname)]][['peakFiles']] <- list()
#             ldt[[as.name(tfname)]][['peakFiles']][[as.name(tissuename)]] <-  paste0(dcids, '_sort_peaks.narrowPeak.bed')
#         }
        
#     } else {
#         ldt[[as.name(tfname)]][['peakFiles']][[as.name(tissuename)]] <-  paste0(dcids, '_sort_peaks.narrowPeak.bed')
#     }
#     }
# }
```

```{r}
factors_motifs <- homerdb %>%
    dplyr::group_by(symbol) %>%
    dplyr::group_split()
ndt <- c()
mdt <- lapply(factors_motifs, function(edt){
    res <- list()
    res[['motifFiles']] <- edt$Filename
    ndt <<- append(ndt, unique(edt$symbol))
    return(res)
})
names(mdt) <- ndt
```

```{r}
common_names <- intersect(names(ldt), names(mdt))
peakslist <- ldt[common_names]
motifslist <- mdt[common_names]
```


```{r}
enpact_models_config <- mapply(c, peakslist, motifslist)
```

```{r}
library(purrr)
# Modified from: https://stackoverflow.com/questions/74655073/how-to-effectively-join-two-lists-elementwise-by-element-name
cat_lists <- function(list1, list2) {  
  keys <- unique(c(names(list1), names(list2)))
  map2(list1[keys], list2[keys], c) |>
    set_names(keys)  

}

enlist <- purrr::reduce(list(motifslist, peakslist), cat_lists)
yaml::write_yaml(enlist, file = '/beagle3/haky/users/temi/projects/TFPred-snakemake/metadata/enpact_models_to_train.yaml')
yaml::write_yaml(enlist[1:10], file = '/beagle3/haky/users/temi/projects/TFPred-snakemake/metadata/enpact_models_to_train.short.yaml')
```

```{r}
minimal_ex <- enlist[c('ASCL2', 'ATF4', 'SREBF2', 'GATA2')]
yaml::write_yaml(minimal_ex, file = '/beagle3/haky/users/temi/projects/TFPred-snakemake/minimal/models.data.yaml')
```

```{r}
minimal_ex <- enlist[c('AR')]
yaml::write_yaml(minimal_ex, file = '/beagle3/haky/users/temi/projects/TFPred-snakemake/metadata/enpact_models_to_train.AR.yaml')
```


```{r}
# prepare the metdata 
mtdt <- sapply(gsub('.peakFiles.', '_', names(rapply(peakslist, function(x) head(x, 1)))), base::strsplit, '_') |> unname() %>% do.call('rbind', .) %>% as.data.frame()
colnames(mtdt) <- c('assay', 'context')
```

```{r}
mtdt %>% dplyr::filter(assay %in% names(minimal_ex)) %>%
    data.table::fwrite(file = '/beagle3/haky/users/temi/projects/TFPred-snakemake/minimal/models.run.tsv', sep = '\t', row.names =F, col.names =T, quote = F)
```

```{r}
data.table::fwrite(mtdt, file = '/project2/haky/temi/projects/TFPred-snakemake/metadata/enpact_models_to_train.tsv', sep = '\t', row.names =F, col.names =T, quote = F)

data.table::fwrite(mtdt[1:10, ], file = '/project2/haky/temi/projects/TFPred-snakemake/metadata/enpact_models_to_train.short.tsv', sep = '\t', row.names =F, col.names =T, quote = F)
```


## Prepare samples for experiment


```{r}

experiment_dir <- '/beagle3/haky/users/temi/projects/TFPred-snakemake/experiments/AR_Prostate'
if(!dir.exists(experiment_dir)){
    dir.create(experiment_dir)
}

minimal_ex <- enlist[c('AR')]
# sselect 20 samples randomly
ar_prostate_samples <- minimal_ex[['AR']][['peakFiles']][['Prostate']]

set.seed(2024)
ar_prostate_samples <- sample(ar_prostate_samples, 50, replace = F) |> sort()

# set minimal_ex to the selected samples
run_dt <- vector('character', length(ar_prostate_samples))
samples_vec <- vector('character', length(ar_prostate_samples))

data_list <- list()

for(i in seq_along(ar_prostate_samples)){
    nnm <- paste0('AR', i)
    data_list[[nnm]] <- list()
    data_list[[nnm]][['motifFiles']] <- minimal_ex[['AR']][['motifFiles']]
    data_list[[nnm]][['peakFiles']] <- list()
    data_list[[nnm]][['peakFiles']][['Prostate']] <- ar_prostate_samples[i]
    nn <- gsub('.narrowPeak.bed', '.data.yaml', ar_prostate_samples[i], fixed = F)
    print(nn); print(data_list)

    run_dt[i] <- nnm
    samples_vec[i] <- nn

}

run_dt <- as.data.frame(run_dt)
run_dt$context <- 'Prostate'
colnames(run_dt) <- c('assay', 'context')

run_dt %>% data.table::fwrite(file = '/beagle3/haky/users/temi/projects/TFPred-snakemake/experiments/AR_Prostate/runs.tsv', sep = '\t', row.names =F, col.names =T, quote = F)

yaml::write_yaml(data_list, file = file.path(experiment_dir, 'models.yaml'))
```


```{r}
print(
        glue("snakemake -s snakefile.smk --configfile experiments/AR_Prostate/pipeline.experiment.yaml --config 'models_config=experiments/AR_Prostate/models.yaml' 'models_metadata=experiments/AR_Prostate/runs.tsv' --profile profiles/simple/ -np > dryrun.out")
    )
# /beagle3/haky/users/temi/projects/TFPred-snakemake/experiments/AR_Prostate/pipeline.experiment.yaml
```


Gather the models

```{r}
models_dir <- '/beagle3/haky/users/temi/projects/TFPred-snakemake/data/EXP_TWENTY_2024-07-22/models'
```
```{r}
#valid_dates <- c('2023-11-14', '2023-12-01')

valid_dates <- c('2024-07-22')

logistic_models <- list.files(models_dir, full.names = TRUE, recursive = T) %>% sapply(., function(each_model){
    if(endsWith(each_model, ".logistic.rds") && any(sapply(valid_dates, grepl, x=each_model, fixed=TRUE))){
        if(file.info(each_model)$size > 0){
            return(each_model)
        }
    }
}) %>% Filter(Negate(is.null), .)
```


```{r}
nn <- names(logistic_models) %>% base::strsplit(., split="/") %>% sapply(., function(each_name){
    each_name[length(each_name)]
}) %>% base::strsplit(., '\\.') %>% sapply(., function(each_name){
    gsub('_Prostate_2024-07-22', '', each_name[1])
})
```


```{r}
if(length(unique(nn)) == length(nn)){
    names(logistic_models) <- nn
    do.call(rbind, logistic_models) %>%
        as.data.frame() %>%
        tibble::rownames_to_column('model') %>%
        setNames(nm=c('model', 'path')) %>%
        data.table::fwrite(., file='/project2/haky/temi/projects/enpact-predict-snakemake/metadata/exp_50_models.txt', row.names=F, col.names=T, quote=F, sep='\t')
}
```



```{r}
list.files('/project2/haky/temi/projects/enpact-predict-snakemake/output/baca_multisample_exp50_2024-07-22/enpact_scores/', pattern ='*.txt.gz', full.names = T, recursive = T) %>% sapply(., function(each_file){
    data.table::fread(each_file) %>% dim()
})
```

### vs. BPNet


```{r}
peaksdir <- '/beagle3/haky/users/temi/data/bpnet-personalized/data'
expdir <- '/beagle3/haky/users/temi/projects/TFPred-snakemake/experiments/bpnet'
```


```{r}
enpact_ar <- enlist[c('AR')]

enpact_ar[['AR']][['peakFiles']] <- NULL
enpact_ar[['AR']][['peakFiles']][['Prostate']] <- 'NKI_13_T.formattedInputPeaks.bed'


```


```{r}
run_dt <- as.data.frame('AR')
run_dt$context <- 'Prostate'
colnames(run_dt) <- c('assay', 'context')

run_dt %>% data.table::fwrite(file = file.path(expdir, 'runs.tsv'), sep = '\t', row.names =F, col.names =T, quote = F)
yaml::write_yaml(enpact_ar, file = file.path(expdir, 'models.yaml'))
```



```{r}
# Author: Temi
# Date: Thursday July 27 2023
# Description: script used to create enformer predict parameters file
# Usage: Rscript create_training_sets.R [OPTIONS]

suppressPackageStartupMessages(library("optparse"))

option_list <- list(
    make_option("--phenotype", help='A GWAS phenotype e.g. alzheimers'),
    make_option("--runname", help="The name of the dataset"),
    make_option("--base_directives", help='A yaml file containing directives for enformer; will be used to create a json file for the enformer predict pipeline'),
    make_option("--project_directory", help='A project directory for enformer predict'),
	make_option("--predictors_file", help='predictor file containing the intervals to predict on'),
    make_option("--model", help='enformer model'),
	make_option("--fasta_file", help='fasta file, typically hg38'),
    make_option("--date", help='fasta file, typically hg38'),
    make_option("--parameters_file", help='the json file that will be created'),
    make_option("--personalized_parameters_file", default=NULL, help='the json file that will be created')
)

opt <- parse_args(OptionParser(option_list=option_list))
print(opt)

opt <- list()
opt$runname <- 'Asthma_GWAS' 
opt$phenotype <- 'asthma_children' 
opt$base_directives <- '/project/haky/users/temi/projects/TFXcan-snakemake/config/enformer_base.yaml' 
opt$project_directory <- '/project/haky/users/temi/projects/TFXcan-snakemake/data'
# --project_directory data --predictors_file data/collection/asthma_children/asthma_children.EnformerLoci.txt --model /project2/haky/Data/enformer/raw --fasta_file /project2/haky/Data/hg_sequences/hg38/Homo_sapiens_assembly38.fasta --parameters_file data/enformer_parameters/enformer_parameters_Asthma_GWAS_asthma_children.json --date 2024-01-31

opt$personalized_parameters_file <- '/project/haky/users/temi/projects/TFXcan-snakemake/config/personalized_base.yaml'



library(yaml)

# read and write the enformer config file
directives <- yaml::yaml.load_file(opt$base_directives)
enformer_parameters_json <- directives$enformer$prediction_directives
# you may change these as appropriate
enformer_parameters_json[['project_dir']] <- normalizePath(opt$project_directory)
enformer_parameters_json[["interval_list_file"]] <- opt$predictors_file
enformer_parameters_json[['prediction_data_name']] <- opt$runname
enformer_parameters_json[['prediction_id']] <- opt$phenotype
enformer_parameters_json[['date']] <- opt$date
enformer_parameters_json[['model_path']] <- opt$model
enformer_parameters_json[['fasta_file']] <- opt$fasta_file

# chANGE the metadata dir
enformer_parameters_json[['metadata_dir']] <- dirname(opt$parameters_file)

# this ensures that personalized parameters are used
if(!is.null(opt$personalized_parameters_file)){
    enformer_parameters_json[['sequence_source']] <- 'personalized'

    # create vcf files patterns and write to a list
    personalized_parameters <- yaml::yaml.load_file(opt$personalized_parameters_file)
    vcf_path <- file.path(personalized_parameters[['vcf_files']][['folder']], personalized_parameters[['vcf_files']][['files_pattern']])

    chrom_filter <- c(1:22)
    valid_chroms <- c()
    chr_vcfs <- sapply(chrom_filter, function(cc){
        pp <- gsub('\\{\\}', cc, vcf_path)
        if(file.exists(pp)){
            valid_chroms <<- append(valid_chroms, cc)
            return(pp)
        }
    }) 

    names(chr_vcfs) <- paste0('chr', valid_chroms)
    chr_vcfs <- as.list(chr_vcfs)
    #chr_vcfs <- Filter(file.exists, chr_vcfs) |> as.list()
    # nn <- paste0('chr', names(chr_vcfs))
    # names(chr_vcfs) <- nn

    enformer_parameters_json[['vcf_files']][['folder']] <- personalized_parameters[['vcf_files']][['folder']]
    enformer_parameters_json[['vcf_files']][['files']] <- chr_vcfs

    enformer_parameters_json[['individuals']] <- personalized_parameters[['individuals']]
    enformer_parameters_json[['n_individuals']] <- personalized_parameters[['n_individuals']]
    enformer_parameters_json[['batch_individuals']] <- personalized_parameters[['batch_individuals']]

}

write(
    jsonlite::toJSON(enformer_parameters_json, na='null', pretty=TRUE, auto_unbox=T),
    file=opt$parameters_file
)


# /beagle3/haky/users/shared_software/TFXcan-pipeline-tools/bin/Rscript prepare/workflow/scripts/create_enformer_config.R --transcription_factor AR --tissue Breast --base_directives config/enformer_base.yaml --project_directory data/predictions_folder --predictors_file data/predictor_files/AR_Breast.predictors.txt --model "/project2/haky/Data/enformer/raw" --fasta_file "/project2/haky/Data/hg_sequences/hg38/Homo_sapiens_assembly38.fasta" --parameters_file data/prediction_parameters/enformer_parameters_cistrome_AR_Breast.json
```


```{r}


pers_params <- list()

pers_params[['sequence_source']] <- 'personalized'
pers_params[['individuals']] <- 'NKI_13'
pers_params[['n_individuals']] <- -1
pers_params[['batch_individuals']] <- -1

vcf_path <- file.path('/project2/haky/Data/baca_cwas/vcfs/hg38/imputed_vcf_hg38_snps_only', 'chr{}.dose.vcf.gz')
chrom_filter <- c(1:22)
valid_chroms <- c()
chr_vcfs <- sapply(chrom_filter, function(cc){
    pp <- gsub('\\{\\}', cc, vcf_path)
    if(file.exists(pp)){
        valid_chroms <<- append(valid_chroms, cc)
        return(pp)
    }
})

chr_vcfs <- basename(chr_vcfs)
names(chr_vcfs) <- paste0('chr', valid_chroms)
chr_vcfs <- as.list(chr_vcfs)

pers_params[['vcf_files']] <- list()
pers_params[['vcf_files']][['folder']] <- '/project2/haky/Data/baca_cwas/vcfs/hg38/imputed_vcf_hg38_snps_only'
pers_params[['vcf_files']][['files']] <- chr_vcfs

# write(
#     jsonlite::toJSON(pers_params, na='null', pretty=TRUE, auto_unbox=T),
#     file='/beagle3/haky/users/temi/projects/TFPred-snakemake/config/personalized_base.yaml'
# )
```


```{r}
yaml::write_yaml(pers_params, file = '/beagle3/haky/users/temi/projects/TFPred-snakemake/config/personalized_base.yaml')
```